{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN10KMKwKHsuEzybOqAv+ta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvaec1030/Data-Science-projects/blob/main/Uber_Data_Analysis_Project_in_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ayushdixit487/Uber-Data-Analysis-Project-in-Pyspark"
      ],
      "metadata": {
        "id": "ZyIRctfPEMSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiUW5mh4Dj24"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"UberDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "df = spark.read.csv(\"uber.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Read the data from CSV file\n",
        "uber = spark.read.csv(\"uber.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Group the data by date and sum the completed trips\n",
        "completed_trips_by_date = uber.groupBy(\"Date\").sum(\"Completed Trips\")\n",
        "\n",
        "# Find the date with the most completed trips\n",
        "date_with_most_completed_trips = completed_trips_by_date \\\n",
        "    .orderBy(\"sum(Completed Trips)\", ascending=False) \\\n",
        "    .select(\"Date\") \\\n",
        "    .first()[\"Date\"]\n",
        "\n",
        "print(date_with_most_completed_trips)\n",
        "\n",
        "#Output:  2012-09-15"
      ],
      "metadata": {
        "id": "usjjKSICDpuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, window\n",
        "\n",
        "# Read the data from CSV file\n",
        "uber = spark.read.csv(\"uber.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Group the data by 24-hour windows and sum the completed trips\n",
        "completed_trips_by_window = uber \\\n",
        "    .groupBy(window(\"Time (Local)\", \"24 hours\")) \\\n",
        "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
        "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
        "\n",
        "# Get the highest number of completed trips within a 24-hour period\n",
        "highest_completed_trips_in_24_hours = completed_trips_by_window \\\n",
        "    .select(\"Total Completed Trips\") \\\n",
        "    .first()[\"Total Completed Trips\"]\n",
        "\n",
        "print(highest_completed_trips_in_24_hours)\n",
        "\n",
        "#Output 2102"
      ],
      "metadata": {
        "id": "jsp_1ZhCDs2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour, sum\n",
        "\n",
        "hourly_requests = df.groupBy(hour(\"Time (Local)\").alias(\"hour\")).agg(sum(\"Requests\").alias(\"total_requests\")).orderBy(\"total_requests\", ascending=False)\n",
        "\n",
        "most_requested_hour = hourly_requests.select(\"hour\").first()[0]\n",
        "print(\"The hour with the most requests is:\", most_requested_hour)\n",
        "\n",
        "#The hour with the most requests is: 17"
      ],
      "metadata": {
        "id": "dci2HBEADxgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dayofweek, hour\n",
        "\n",
        "weekend_zeros = df.filter((hour(\"Time (Local)\") >= 17) | (hour(\"Time (Local)\") < 3)).filter((dayofweek(\"Date\") == 6) | (dayofweek(\"Date\") == 7)).agg(sum(\"Zeroes\").alias(\"weekend_zeros\")).collect()[0][\"weekend_zeros\"]\n",
        "\n",
        "total_zeros = df.agg(sum(\"Zeroes\").alias(\"total_zeros\")).collect()[0][\"total_zeros\"]\n",
        "\n",
        "percent_weekend_zeros = weekend_zeros / total_zeros * 100\n",
        "\n",
        "print(\"The percentage of zeros that occurred on weekends is:\", percent_weekend_zeros, \"%\")\n",
        "\n",
        "#The percentage of zeros that occurred on weekends is: 41.333414829040026 %"
      ],
      "metadata": {
        "id": "-MPAOiEwD0zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "weighted_avg = df.withColumn(\"completed_per_driver\", df[\"Completed Trips\"] / df[\"Unique Drivers\"]) \\\n",
        "                 .groupBy(\"Date\", \"Time (Local)\") \\\n",
        "                 .agg(avg(\"completed_per_driver\").alias(\"avg_completed_per_driver\"), sum(\"Completed Trips\").alias(\"total_completed_trips\")) \\\n",
        "                 .withColumn(\"weighted_ratio\", col(\"avg_completed_per_driver\") * col(\"total_completed_trips\")) \\\n",
        "                 .agg(sum(\"weighted_ratio\") / sum(\"total_completed_trips\")).collect()[0][0]\n",
        "\n",
        "print(\"The weighted average ratio of completed trips per driver is:\", weighted_avg)\n",
        "\n",
        "#Output: The weighted average ratio of completed trips per driver is: 1.2869201507713425"
      ],
      "metadata": {
        "id": "qEVKjVopD4U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, hour, countDistinct\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate the number of unique requests for each hour of the day\n",
        "hourly_unique_requests = (df\n",
        "  .groupBy(hour(\"Time (Local)\").alias(\"hour\"))\n",
        "  .agg(countDistinct(\"Requests\").alias(\"unique_requests\"))\n",
        ")\n",
        "\n",
        "# Slide a window of 8 hours to find the busiest 8 consecutive hours\n",
        "window = Window.orderBy(col(\"unique_requests\").desc()).rowsBetween(0, 7)\n",
        "busiest_8_consecutive_hours = (hourly_unique_requests\n",
        "  .select(\"*\", sum(\"unique_requests\").over(window).alias(\"consecutive_sum\"))\n",
        "  .orderBy(col(\"consecutive_sum\").desc())\n",
        "  .limit(1)\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "busiest_8_consecutive_hours.show()"
      ],
      "metadata": {
        "id": "Ki8qs2ZqD7cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Group the data by 72-hour periods and calculate the ratio of zeroes to eyeballs for each period\n",
        "period_ratios = (df\n",
        "  .groupBy(((col(\"Date\").cast(\"timestamp\").cast(\"long\") / (72*3600)).cast(\"int\")).alias(\"period\"))\n",
        "  .agg(sum(\"Zeroes\").alias(\"zeroes\"), sum(\"Eyeballs\").alias(\"eyeballs\"))\n",
        "  .withColumn(\"ratio\", col(\"zeroes\") / col(\"eyeballs\"))\n",
        ")\n",
        "\n",
        "# Find the period with the highest ratio\n",
        "highest_ratio_period = period_ratios.orderBy(col(\"ratio\").desc()).limit(1)\n",
        "\n",
        "# Print the result\n",
        "highest_ratio_period.show()"
      ],
      "metadata": {
        "id": "X_sQlbX0D-xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate requests per unique driver for each hour\n",
        "requests_per_driver = (df.groupBy('Time (Local)').agg(\n",
        "    (F.sum('Requests') / F.countDistinct('Unique Drivers')).alias('requests_per_driver'))\n",
        ")\n",
        "\n",
        "# Show the hour with the highest ratio\n",
        "requests_per_driver.orderBy(F.desc('requests_per_driver')).show(1)"
      ],
      "metadata": {
        "id": "_Zh5drQIEEM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average completed trips and unique drivers for each hour\n",
        "avg_trips_and_drivers = (df.groupBy('Time (Local)').agg(\n",
        "    F.mean('Completed Trips').alias('avg_completed_trips'),\n",
        "    F.mean('Unique Drivers').alias('avg_unique_drivers')\n",
        "))\n",
        "\n",
        "# Show the hour with the lowest average completed trips and unique drivers\n",
        "avg_trips_and_drivers.orderBy('avg_completed_trips', 'avg_unique_drivers').show(1)\n"
      ],
      "metadata": {
        "id": "pWKyltMtEHKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}